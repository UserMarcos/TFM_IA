{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c22c9e4-aa1e-42d5-8ac0-df4bbce089ef",
   "metadata": {},
   "source": [
    "# Pruebas con la librería __Torch__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4c57dd-29f0-40af-ade5-89642d795bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f3937-2614-4c20-9e0b-f6b70cb5d6ba",
   "metadata": {},
   "source": [
    "- Cargamos los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f645fe7b-cc31-4638-83cd-093c951b2851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando cuda como dispositivo\n"
     ]
    }
   ],
   "source": [
    "# Bajamos los datos de entrenamiento del conjunto de datos abierto.\n",
    "datos_entrenamiento = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Bajamos los datos de test del conjunto de datos abierto.\n",
    "datos_test = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "clases = [\n",
    "    \"Camiseta/top\",\n",
    "    \"Pantalón\",\n",
    "    \"Jersey\",\n",
    "    \"Vestido\",\n",
    "    \"Abrigo\",\n",
    "    \"Sandalia\",\n",
    "    \"Camisa\",\n",
    "    \"Zapatillas\",\n",
    "    \"Bolso\",\n",
    "    \"Botín\",\n",
    "]\n",
    "\n",
    "dispositivo = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Usando {dispositivo} como dispositivo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e6a484-40db-4a9c-bf9c-b2939ad74322",
   "metadata": {},
   "source": [
    "- Examinamos el `dataset` y sus `items`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ff4441e-a521-46b9-8eb0-c4021ef5b9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº de items del dataset: 60000\n",
      "Tipo del item: <class 'tuple'>\n",
      "Dimensiones de la tupla: 2\n",
      "Tipo del item[0]: <class 'torch.Tensor'>\n",
      "Dimensiones del tensor item[0]: torch.Size([1, 28, 28])\n",
      "Tipo de dato del tensor item[0]: torch.float32\n",
      "Tipo de dispositivo donde esta almacenado el tensor item[0]: cpu\n",
      "Tipo de dispositivo donde esta almacenado el tensor 'tensor': cuda:0\n",
      "Tipo del item[1]: <class 'int'>\n",
      "Valor de item[1]: 0 - Camiseta/top\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mostrar_imagen(imagen_mostrar):\n",
    "    # Mostrar imagen\n",
    "    plt.imshow(imagen_mostrar, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "n = len(datos_entrenamiento)\n",
    "print(f\"Nº de items del dataset: {n}\")\n",
    "item = datos_entrenamiento[1]\n",
    "tensor = item[0].to(dispositivo)\n",
    "print(f\"Tipo del item: {type(item)}\")\n",
    "print(f\"Dimensiones de la tupla: {len(item)}\")\n",
    "print(f\"Tipo del item[0]: {type(item[0])}\")\n",
    "print(f\"Dimensiones del tensor item[0]: {item[0].shape}\")\n",
    "print(f\"Tipo de dato del tensor item[0]: {item[0].dtype}\")\n",
    "print(f\"Tipo de dispositivo donde esta almacenado el tensor item[0]: {item[0].device}\")\n",
    "print(f\"Tipo de dispositivo donde esta almacenado el tensor 'tensor': {tensor.device}\")\n",
    "print(f\"Tipo del item[1]: {type(item[1])}\")\n",
    "print(f\"Valor de item[1]: {item[1]} - {clases[item[1]]}\")\n",
    "\n",
    "# Cambiar el orden de los ejes a H x W x C para Matplotlib\n",
    "image_tensor = item[0].permute(1, 2, 0)\n",
    "\n",
    "mostrar_imagen(image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a66f04-550b-4dce-b71a-9edbe4acfd29",
   "metadata": {},
   "source": [
    "- Cargar el `dataset`al `dataloader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa84f4f0-d8ee-4318-918e-d0993c74d188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Dimensiones de y: torch.Size([64]) torch.int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tamanho_batch = 64\n",
    "\n",
    "# Creamos los data loaders.\n",
    "train_dataloader = DataLoader(datos_entrenamiento, batch_size=tamanho_batch)\n",
    "test_dataloader = DataLoader(datos_test, batch_size=tamanho_batch)\n",
    "\n",
    "\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Dimensiones de X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Dimensiones de y: {y.shape} {y.dtype}\")\n",
    "    image_tensor = X[0, :, :, :].permute(1, 2, 0)\n",
    "    mostrar_imagen(image_tensor)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020fa783-491b-4e46-8907-1c440d385a0a",
   "metadata": {},
   "source": [
    "- Definimos la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa67729-fd18-4235-9993-e1b939f4c0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "modelo = NeuralNetwork().to(dispositivo)\n",
    "print(modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fbcf62-3b7b-41f4-b33c-6628eff2f76a",
   "metadata": {},
   "source": [
    "- Definimos la _funcion de pérdida_ y el _optimizador_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baea909c-68ee-470f-b859-49b0c1ab2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "funcion_perdida = nn.CrossEntropyLoss()\n",
    "optimizador = torch.optim.SGD(modelo.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247245dd-0370-447b-b7cb-8766dcfd4240",
   "metadata": {},
   "source": [
    "- Definimos el bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92371f20-913b-4457-82e3-173698879fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Calculamos el error de predicción\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"perdida: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d06e80-084d-49ab-8c6d-20dec4a34794",
   "metadata": {},
   "source": [
    "- Definimos el bucle de _test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a5aa540-9793-47eb-895d-f2d0e7983c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Precisión: {(100*correct):>0.1f}%, perdida media: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec289e-13e0-42d2-a5de-39ae3ba9e1d2",
   "metadata": {},
   "source": [
    "- Ejecutamos las épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57670be0-1db6-4a56-b533-619453674275",
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas = 5\n",
    "for t in range(epocas):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, modelo, funcion_perdida, optimizador)\n",
    "    test(test_dataloader, modelo, funcion_perdida)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda3be8-c394-4905-8b34-b29137be2919",
   "metadata": {},
   "source": [
    "- Guardamos el modelos en disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03c8520-3aae-4eac-be72-bf2617f8e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelo.state_dict(), \"modelo.pth\")\n",
    "print(\"Guardado el 'PyTorch Model State' en modelo.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb27247-ebd8-4254-95a0-65528be20bae",
   "metadata": {},
   "source": [
    "- Cargamos el modelo de disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58c4e1cc-b475-4356-9a32-6c8a8c8a8b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(dispositivo)\n",
    "model.load_state_dict(torch.load(\"modelo.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987e8750-29b4-450d-bd0b-9463429d6f47",
   "metadata": {},
   "source": [
    "- Hacemos predicciones sobre el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19141ff1-94ab-4fb3-aeb0-940529c89e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicho: \"Botín\", Real: \"Botín\"\n",
      "tensor([[-3.9287, -5.4405, -2.0929, -2.3425, -1.6020,  5.0782, -1.8001,  4.5368,\n",
      "          2.7817,  5.3729]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "x, y = datos_test[0][0], datos_test[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(dispositivo)\n",
    "    pred = model(x)\n",
    "    predicted, actual = clases[pred[0].argmax(0)], clases[y]\n",
    "    print(f'Predicho: \"{predicted}\", Real: \"{actual}\"')\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec65a28-1b4c-4e96-98ab-8f4dd67ee9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
